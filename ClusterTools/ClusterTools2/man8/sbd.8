.TH sbd 8 "10 Oct 2010" "" "cluster-glue"
.\"
.SH NAME
sbd \- Split Brain Detector deamon
.\"
.SH SYNOPSIS
.B sbd
\fIOPTIONS\fR [\fIOPT_ARGUMENT\fR] [\fICOMMAND\fR] [\fICMD_ARGUMENT\fR]

.\"
.SH OPTIONS
.TP
\fB-d\fR <SBD>
	Block device to use (mandatory)
.TP
\fB-h\fR
	Display this help.
.TP
\fB-n\fR <node>
	Set local node name; defaults to uname -n (optional)
.TP
\fB-R\fR
	Do NOT enable realtime priority (debugging only)
.TP
\fB-W\fR
	Use watchdog (recommended) (watch only)
.TP
\fB-w\fR <SBD>
	Specify watchdog device (optional) (watch only)
.TP
\fB-D\fR
	Run as background daemon (optional) (watch only)
.TP
\fB-v\fR
	Enable some verbose debug logging (optional)
.TP
\fB-1\fR <N>
	Set watchdog timeout to N seconds (optional) (create only)
.TP
\fB-2\fR <N>
	Set slot allocation timeout to N seconds (optional) (create only
)
.TP
\fB-3\fR <N>
	Set daemon loop timeout to N seconds (optional) (create only)
.TP
\fB-4\fR <N>
	Set msgwait timeout to N seconds (optional) (create only)
.TP
\fB-5\fR <N>
	Warn if loop latency exceeds threshold (optional) (watch only)
(default is 3, set to 0 to disable)

.\"
.SH COMMAND
.TP
\fBcreate\fR
	Initialize N slots on devicde <SBD> - OVERWRITES DEVICE!
.TP
\fBlist\fR
	List all allocated slots on device, and messages.
.TP
\fBdump\fR
	Dump meta-data header from device.
.TP
\fBwatch\fR        
	Loop forever, monitoring own slot
.TP
\fBallocate\fR <node>
	Allocate a slot for node (optional)
.TP
\fBmessage\fR <node> (test|reset|off|clear|exit)
	Write the specified message to node's slot.

.\"
.SH DESCRIPTION

The \fBsbd\fR daemon automatically allocates one of the message slots on the
assigned disk partition to itself, and constantly monitors it for messages to
itself.
Upon receipt of a message, the daemon immediately complies with the
request, such as initiating a power-off or reboot cycle for fencing.

The daemon also constantly monitors connectivity to the storage device,
and commits suicide in case the partition becomes unreachable,
guaranteeing that it is not disconnected from fencing message.

The daemon is brought online on each node before the rest of the
cluster stack is started, and terminated only after all other cluster
components have been shut down - ensuring that cluster resources are
never activated without SBD supervision.

The environment must have shared storage reachable by all nodes.
This shared storage segment must not make use of host-based RAID, cLVM2,
nor DRBD. Please refer to sbd(7) for more information.

The \fBsbd\fR can also be called manually to perform actions described in the 
commands section of this manual page.

In the rest of this text, this is referred to as "/dev/<SBD>", please substitute
your actual pathname (ie, "/dev/sdc1") for this below.

.\"
.SH EXAMPLES


\fB* Initialising SBD Partition\fR

All these steps must be performed as root.

After having made very sure that this is indeed the device you want to
use, and does not hold any data you need - as the sbd command will
overwrite it without further requests for confirmation -, initialize the
sbd device:

# \fBsbd -d /dev/<SBD> create\fR

This will write a header to the device, and create slots for up to 255
nodes sharing this device with default timings.

If your sbd device resides on a multipath group, you may need to adjust
the timeouts sbd uses, as MPIO's path down detection can cause some
latency: after the msgwait timeout, the message is assumed to have been
delivered to the node. For multipath, this should be the time required
for MPIO to detect a path failure and switch to the next path. You may
have to test this in your environment.


\fB* Setting Watchdog Timeout for SBD Partition\fR

The node will perform suicide if
it has not updated the watchdog timer fast enough; the watchdog timeout
must be shorter than the msgwait timeout - half the value is a good
estimate. This can be specified when the SBD device is initialized:

# \fB/usr/sbin/sbd -d /dev/<SBD> -4 $msgwait -1 $watchdogtimeout create\fR

(All timeouts are in seconds.)


\fB* Dumping Content of SBD Partition\fR

You can look at what was written to the device using:

# \fBsbd -d /dev/<SBD> dump\fR 
.br
Header version     : 2
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10

As you can see, the timeouts are also stored in the header, to ensure
that all participating nodes agree on them.

Additionally, it is highly recommended that you set up your Linux system
to use a watchdog.


\fB* Starting the SBD daemon\fR

The sbd daemon is a critical piece of the cluster stack. It must always
be running when the cluster stack is up, or even when the rest of it has
crashed, so that it can be fenced.

The openais init script starts and stops SBD if configured; add the
following to /etc/sysconfig/sbd:

===
.br
SBD_DEVICE="/dev/<SBD>"
.br
# The next line enables the watchdog support:
.br
SBD_OPTS="-W"
.br
=== 

Before proceeding, ensure that SBD has indeed started on all nodes
through
# \fBrcopenais restart\fR


\fR* Listing Content of SBD\fR

The command

# \fBsbd -d /dev/<SBD> list\fR

will dump the node slots, and their current messages, from the sbd
device. You should see all cluster nodes that have ever been started
with sbd being listed there; most likely with the message slot showing
"clear".


\fR* Testing SBD\fR

You can now try sending a test message to one of the nodes:

# \fBsbd -d /dev/<SBD> message nodea test\fR

The node will acknowledge the receipt of the message in the system logs:
.br
Aug 29 14:10:00 nodea sbd: [13412]: info: Received command test from nodeb

This confirms that SBD is indeed up and running on the node, and that it
is ready to receive messages.


\fB* Configuring the Fencing Resource in the Cluster Information Base\fR

To complete the sbd setup, it is necessary to activate sbd as a
STONITH/fencing mechanism in the CIB as follows:

# \fBcrm
.br
configure
.br
property stonith-enabled="true"
.br
property stonith-timeout="30s"
.br
primitive stonith:external/sbd params sbd_device="/dev/<SBD>"
.br
commit
.br
quit
\fR

Note that since node slots are allocated automatically, no manual
hostlist needs to be defined.

Once the resource has started, your cluster is now successfully
configured for shared-storage fencing, and will utilize this method in
case a node needs to be fenced.
.\"
.SH BUGS
To report bugs for a SUSE or Novell product component, please use
 http://support.novell.com/additional/bugreport.html .
.\"
.SH SEE ALSO

\fBsbd\fP(7),
http://www.linux-ha.org/wiki/SBD_Fencing ,
http://www.mail-archive.com/pacemaker@oss.clusterlabs.org/msg03849.html ,
http://www.novell.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/part_config.html
.\"
.SH AUTHORS
The content of this manual page was mostly derived from online documentation
mentioned above and the programm's help option.
.\"
.SH COPYRIGHT
(c) 2009-2010 SUSE Linux GmbH, Germany.
.br
sbd comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
