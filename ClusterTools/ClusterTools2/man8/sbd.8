.TH sbd8 "10 Oct 2010" "" "cluster-glue"
.\"
.SH NAME
sbd \- Split Brain Detector
.\"

.SH DESCRIPTION


==== Requirements ====

The environment must have shared storage reachable by all nodes. It is
recommended to create a 1MB partition at the start of the device; in the
rest of this text, this is referred to as "/dev/SBD", please substitute
your actual pathname (ie, "/dev/sdc1") for this below.

This shared storage segment must not make use of host-based RAID, cLVM2,
nor DRBD.

However, using storage-based RAID and multipathing is recommended for
increased reliability.

==== SBD partition ====

All these steps must be performed as root.

After having made very sure that this is indeed the device you want to
use, and does not hold any data you need - as the sbd command will
overwrite it without further requests for confirmation -, initialize the
sbd device:

# sbd -d /dev/SBD create

This will write a header to the device, and create slots for up to 255
nodes sharing this device with default timings.

If your sbd device resides on a multipath group, you may need to adjust
the timeouts sbd uses, as MPIO's path down detection can cause some
latency: after the msgwait timeout, the message is assumed to have been
delivered to the node. For multipath, this should be the time required
for MPIO to detect a path failure and switch to the next path. You may
have to test this in your environment. The node will perform suicide if
it has not updated the watchdog timer fast enough; the watchdog timeout
must be shorter than the msgwait timeout - half the value is a good
estimate. This can be specified when the SBD device is initialized:

# /usr/sbin/sbd -d /dev/SBD -4 $msgwait -1 $watchdogtimeout create

(All timeouts are in seconds.)

You can look at what was written to the device using:

# sbd -d /dev/SBD dump 
Header version     : 2
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10

As you can see, the timeouts are also stored in the header, to ensure
that all participating nodes agree on them.

==== Setup the software watchdog ====

Additionally, it is highly recommended that you set up your Linux system
to use a watchdog. Please refer to the SLES manual for this step(?).

This involves loading the proper watchdog driver on system boot. On HP
hardware, this is the "hpwdt" module. For systems with a Intel TCO,
"iTCO_wdt" can be used. "softdog" is the most generic driver, but it is
recommended that you use one with actual hardware integration. (See
"drivers/watchdog" in the kernel package for a list of choices.)

==== Starting the sbd daemon ====

The sbd daemon is a critical piece of the cluster stack. It must always
be running when the cluster stack is up, or even when the rest of it has
crashed, so that it can be fenced.

The openais init script starts and stops SBD if configured; add the
following to /etc/sysconfig/sbd:

===
SBD_DEVICE="/dev/SBD"
# The next line enables the watchdog support:
SBD_OPTS="-W"
=== 

If the SBD device is not accessible, the daemon will fail to start and
inhibit openais startup.

Note: If the SBD device becomes inaccessible from a node, this could
cause the node to enter an infinite reboot cycle. That is technically
correct, but depending on your administrative policies, might be a
considered a nuisance. You may wish to not automatically start up
openais on boot in such cases.

Before proceeding, ensure that SBD has indeed started on all nodes
through "rcopenais restart".

=== Testing SBD ===

The command

# sbd -d /dev/SBD list

Will dump the node slots, and their current messages, from the sbd
device. You should see all cluster nodes that have ever been started
with sbd being listed there; most likely with the message slot showing
"clear".

You can now try sending a test message to one of the nodes:

# sbd -d /dev/SBD message nodea test

The node will acknowledge the receipt of the message in the system logs:

Aug 29 14:10:00 nodea sbd: [13412]: info: Received command test from nodeb

This confirms that SBD is indeed up and running on the node, and that it
is ready to receive messages.


==== Configuring the fencing resource ====

To complete the sbd setup, it is necessary to activate sbd as a
STONITH/fencing mechanism in the CIB as follows:

# crm
configure
property stonith-enabled="true"
property stonith-timeout="30s"
primitive stonith:external/sbd params sbd_device="/dev/SBD"
commit
quit

Note that since node slots are allocated automatically, no manual
hostlist needs to be defined.

The SBD mechanism is used instead of other fencing/stonith mechanisms;
please disable any others you might have configured before.

Once the resource has started, your cluster is now successfully
configured for shared-storage fencing, and will utilize this method in
case a node needs to be fenced.


.\"
.SH SYNOPSIS
.B sbd

.\"
.SH OPTIONS

.\"
.SH EXAMPLES
.TP


.\"
.SH SEE ALSO

\fBsbd\fP(7),

http://www.linux-ha.org/wiki/SBD_Fencing ,
http://www.mail-archive.com/pacemaker@oss.clusterlabs.org/msg03849.html ,
http://www.novell.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/part_config.html

.\"
.SH COPYRIGHT
(c) 2009-2010 SUSE Linux GmbH, Germany.
.br
sbd comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
